{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icenicee/TP01/blob/main/pw4_gradient_descent_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f17ceb",
      "metadata": {
        "id": "b8f17ceb"
      },
      "source": [
        "# Practical Work 4: Introduction to Gradient Descent with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba79f28c",
      "metadata": {
        "id": "ba79f28c"
      },
      "source": [
        "<br/><br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "68483d4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68483d4e",
        "outputId": "6bc80bd9-6f58-42a4-e94f-5c98305b71b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ea05b0",
      "metadata": {
        "id": "19ea05b0"
      },
      "source": [
        "We will find the minimum of the function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined by\n",
        "$$ f(x) = f(x_0,x_1) = x_0^2 + x_1^2 - \\sin(2x_0)x_1 \\quad .$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb317301",
      "metadata": {
        "id": "eb317301"
      },
      "outputs": [],
      "source": [
        "nr,nc = 256,256\n",
        "a = 3\n",
        "extent = ((-a-0.5/nc, a-0.5/nc, -a-0.5/nr, a-0.5/nr))\n",
        "xs = np.linspace(a, -a, nr)\n",
        "ys = np.linspace(-a, a, nc)\n",
        "xm, ym = np.meshgrid(xs, ys, indexing='ij')\n",
        "xm = xm.T\n",
        "ym = ym.T\n",
        "\n",
        "y = xm**2 + ym**2 - np.sin(2*xm)*ym\n",
        "\n",
        "# other choices (just for fun):\n",
        "#   y = np.sqrt(1+xm**2 + ym**2 - np.sin(xm*2)*ym)\n",
        "#   y = 2*ym**2-np.cos(xm*3)*ym + 2*xm**2\n",
        "\n",
        "fig = plt.figure(dpi=150)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.imshow(y,cmap = 'gray', extent=extent)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8d559c",
      "metadata": {
        "id": "1e8d559c"
      },
      "source": [
        "**QUESTION :**\n",
        "\n",
        "- Define the function $f$ in Pytorch: `f`should take a torch.tensor `x` of shape $(2)$ as input and should output $f(x)$.\n",
        "- Test the function $f$ by computing $f(x)$ for $x=(0,1)$.\n",
        "- Compute its gradient at point $x=(0,1)$ with Pytorch.\n",
        "- Can you check the obtained values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863b241d",
      "metadata": {
        "id": "863b241d"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    # ...\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf5c153",
      "metadata": {
        "id": "7cf5c153"
      },
      "source": [
        "**QUESTION :** Look at the following cell, which implements gradient descent (with fixed step size $\\tau$).\n",
        "\n",
        "Try to understand each line of this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df5de18",
      "metadata": {
        "id": "6df5de18"
      },
      "outputs": [],
      "source": [
        "\n",
        "y = xm**2 + ym**2 - np.sin(xm*2)*ym   # copy the formula to display function values in background\n",
        "\n",
        "x0 = np.array([-1.5,3.])\n",
        "x = torch.tensor(x0, requires_grad=True)\n",
        "\n",
        "tau = 0.1\n",
        "N = 1000\n",
        "xd = np.zeros((N,2))\n",
        "\n",
        "fxlist = []\n",
        "\n",
        "for n in range(N):\n",
        "    fx = f(x)\n",
        "    fx.backward()\n",
        "    with torch.no_grad():\n",
        "        x -= tau*x.grad\n",
        "    x.grad.zero_()\n",
        "    xd[n,:] = x.detach().cpu()\n",
        "    fxlist.append(fx.item())\n",
        "\n",
        "fig = plt.figure(dpi=150)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.imshow(y,cmap = 'gray', extent=extent)\n",
        "plt.scatter(x0[0], x0[1],c='red',alpha=.5)\n",
        "plt.scatter(xd[:, 0], xd[:,1],c='deepskyblue',alpha=.5)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "plt.plot(fxlist)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe905499",
      "metadata": {
        "id": "fe905499"
      },
      "source": [
        "**QUESTION:** Compare by doing gradient descent with the Pytorch routine ``torch.optim.SGD`` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6209363",
      "metadata": {
        "id": "b6209363"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(x0, requires_grad=True)\n",
        "optim = torch.optim.SGD([x], lr=tau)\n",
        "losslist = []\n",
        "for it in range(N):\n",
        "    loss = f(x)\n",
        "    losslist.append(loss.item())\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964b33dc"
      },
      "source": [
        "# Task\n",
        "Implement the PyTorch function `f(x) = x_0^2 + x_1^2 - sin(2x_0)x_1` for a `torch.Tensor` input `x`, compute its value and gradient at `x=(0,1)`, and print the gradient."
      ],
      "id": "964b33dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e49fcebb"
      },
      "source": [
        "## Define Pytorch Function f\n",
        "\n",
        "### Subtask:\n",
        "Implement the mathematical function f(x) = x_0^2 + x_1^2 - sin(2x_0)x_1 using PyTorch operations within a Python function `f(x)` that takes a torch.Tensor as input.\n"
      ],
      "id": "e49fcebb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bda95392"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the function `f(x)` using PyTorch operations as defined in the subtask instructions, taking `x` as a `torch.Tensor` and returning the computed value.\n",
        "\n"
      ],
      "id": "bda95392"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3735704a",
        "outputId": "3a3e9419-a940-417c-d74c-8fc4320bb339"
      },
      "source": [
        "def f(x):\n",
        "    x0 = x[0]\n",
        "    x1 = x[1]\n",
        "    return x0**2 + x1**2 - torch.sin(2*x0)*x1\n",
        "\n",
        "# Test the function with x=(0,1)\n",
        "x_test = torch.tensor([0.0, 1.0], requires_grad=True)\n",
        "f_value = f(x_test)\n",
        "print(f\"f({x_test.tolist()}) = {f_value.item()}\")\n",
        "\n",
        "# Compute its gradient at point x=(0,1)\n",
        "f_value.backward()\n",
        "gradient = x_test.grad\n",
        "print(f\"Gradient of f at {x_test.tolist()} = {gradient.tolist()}\")\n",
        "\n",
        "# Manual check for gradient at (0,1):\n",
        "# f(x0, x1) = x0^2 + x1^2 - sin(2x0)x1\n",
        "# df/dx0 = 2*x0 - cos(2x0)*2*x1\n",
        "# df/dx1 = 2*x1 - sin(2x0)\n",
        "# At (0,1):\n",
        "# df/dx0 = 2*0 - cos(2*0)*2*1 = 0 - 1*2 = -2\n",
        "# df/dx1 = 2*1 - sin(2*0) = 2 - 0 = 2\n",
        "# Expected gradient: [-2.0, 2.0]"
      ],
      "id": "3735704a",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f([0.0, 1.0]) = 1.0\n",
            "Gradient of f at [0.0, 1.0] = [-2.0, 2.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b2c6da1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The gradient of the function $f(x) = x_0^2 + x_1^2 - \\sin(2x_0)x_1$ at $x=(0,1)$ is $[-2.0, 2.0]$.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The PyTorch function $f(x) = x_0^2 + x_1^2 - \\sin(2x_0)x_1$ was successfully implemented.\n",
        "*   At the input $x=[0.0, 1.0]$, the function $f(x)$ evaluates to $1.0$.\n",
        "*   The gradient of $f(x)$ at $x=[0.0, 1.0]$ was computed as $[-2.0, 2.0]$, matching manual calculations.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   This demonstrates the seamless integration of symbolic differentiation within PyTorch's automatic differentiation engine for complex functions.\n"
      ],
      "id": "8b2c6da1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}